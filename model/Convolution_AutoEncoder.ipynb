{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "#DATA LOADING\n",
    "ds = pd.read_csv('/Users/tanish/Downloads/DATA/all/train.csv').values\n",
    "print ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28)\n",
      "(10000,)\n",
      "(1000, 1, 28, 28)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPARATION\n",
    "xtrain = ds[:10000 , 1:].reshape((-1,1,28,28)) / 255.0\n",
    "ytrain = ds[:10000 , 0]\n",
    "print xtrain.shape\n",
    "print ytrain.shape\n",
    "\n",
    "xtest = ds[10000:11000 , 1:].reshape((-1,1,28,28)) / 255.0\n",
    "ytest = ds[10000:11000 , 0]\n",
    "print xtest.shape\n",
    "print ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArchitecture:\\n\\nEncoder -> linear(converting to 2d) -> linear(converting from 2d) -> Decoder\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Architecture:\n",
    "\n",
    "Encoder -> linear(converting to 2d) -> linear(converting from 2d) -> Decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING CLASS\n",
    "#CAE = convolution auto encoder\n",
    "class CAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE , self).__init__()#Why??\n",
    "        \n",
    "        #1.\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Conv2d(1 , 16 , 3 , stride=3 , padding=1),# b, 16, 10, 10\n",
    "        #1=channel , 16= no. of kernels , 3= kernel size\n",
    "        #result will be 16 channel , 10x10 image\n",
    "            \n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        nn.MaxPool2d(2 , stride=2),# b, 16, 5, 5\n",
    "        #just decresing the size using pool operation result=>16 channel 5x5\n",
    "            \n",
    "        \n",
    "        nn.Conv2d(16 , 8 , 3 , stride=2 , padding=1),  # b, 8, 3, 3\n",
    "        #16 channel , 8 is no. of kernels ,size of kernel is 3\n",
    "        #result will be 8 channel , 3x3 image\n",
    "            \n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        nn.MaxPool2d(2 , stride=1)# b, 8, 2, 2\n",
    "        #result 8 channel 2x2image\n",
    "        )\n",
    "        \n",
    "        #2.\n",
    "        '''self.decoder = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8 , 16 , 3 , stride=2),  # b, 16, 5, 5\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(16 , 8 , 5 , stride=3 , padding=1),  # b, 8, 15, 15\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(8 , 1, 2 , stride=2 , padding=1),   # b, 1, 28, 28\n",
    "        nn.Sigmoid()#for ranging b/w 0 to 1\n",
    "        )'''\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.ConvTranspose2d(8 , 16 , 4 , stride=3), # b ,16 ,7 ,7\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(16 , 1, 4 , stride=4), #b , 1, 28,28\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "        '''\n",
    "        We can also apply linearity b/w convolutions\n",
    "        '''\n",
    "        \n",
    "        #3.\n",
    "        self.encode_linear = nn.Linear(32,2)\n",
    "        #converting to 2d \n",
    "        #32 came from 8x2x2\n",
    "        #2 means 2d\n",
    "        \n",
    "        #4.\n",
    "        self.decode_linear = nn.Linear(2,32)\n",
    "        #converting 2d to 32 dimensions\n",
    "        \n",
    "    \n",
    "    def forward(self , x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self ,x):\n",
    "        x = self.encoder(x)\n",
    "        #Befor passing it to linear function weneedto reshape it\n",
    "        #which can be done by view function\n",
    "        x = x.view(x.size(0) , -1)\n",
    "        x = self.encode_linear(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self , x):\n",
    "        #First we need to convert linearity\n",
    "        x = self.decode_linear(x)\n",
    "        #now we need to change the shape\n",
    "        x = x.view(x.size(0) , 8 , 2 , 2)#??How\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING CLASS OBJECT\n",
    "model = CAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING LOSS AND OPTIMIZER FUNCTION\n",
    "loss_func = nn.MSELoss()\n",
    "#as we are predicting input only so loss. would be MSE\n",
    "\n",
    "optimizer  = torch.optim.Adam(model.parameters() , lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING BATCHES\n",
    "def make_batch(train ,label , batch_size=10):\n",
    "    \n",
    "    start = 0\n",
    "    stop = start + batch_size\n",
    "    while start<train.shape[0]:\n",
    "        yield Variable(torch.FloatTensor(train[start:stop]) , requires_grad=True) , Variable(torch.FloatTensor(train[start:stop]))\n",
    "        #?? why this float and long??\n",
    "        #why not label in 2nd part??\n",
    "    \n",
    "        start = stop\n",
    "        stop = start + batch_size\n",
    "#error in using long tensor in second arg  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: ', 0, '| Step: ', 0, '| Loss: ', array(0.05301737, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "#TRAINING STARTS\n",
    "for epoch in range(1):\n",
    "    for step, (bx , by) in enumerate(make_batch(xtrain , ytrain , 256)):#why 256??\n",
    "        #print step,#??  ans -> 10000/256==39.06\n",
    "        \n",
    "        #predicting\n",
    "        output = model(bx)\n",
    "        #print output[0]\n",
    "        #print by[0]\n",
    "        \n",
    "        #Loss calculation\n",
    "        loss = loss_func(output , by)#what if by,output??\n",
    "        #print bx.shape\n",
    "        #print by.shape\n",
    "        \n",
    "        #Equating allgrads to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #calculating grads\n",
    "        loss.backward()\n",
    "        \n",
    "        #applying gradients and lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if step % 200 == 0:#toprint oncein one epoch we have usedif so that we canalso print multiple times in a epoch\n",
    "            test_output = model(torch.FloatTensor(xtest))\n",
    "            test_loss = loss_func(test_output, torch.FloatTensor(xtest))\n",
    "            ls = test_loss.cpu().data.numpy()\n",
    "            a = test_output.data.numpy()\n",
    "            \n",
    "            #acc = (a == xtest).sum()/xtest.shape\n",
    "            # pred_y = torch.max(test_output, 1)[1].data.squeeze().numpy()\n",
    "            # accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| Step: ', step, '| Loss: ', ls )\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(8, 16, kernel_size=(4, 4), stride=(3, 3))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (encode_linear): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (decode_linear): Linear(in_features=2, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(torch.FloatTensor(xtrain[:100])).data.numpy()#??what r we doing here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x109f00410>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADNhJREFUeJzt3X+IXPW5x/HPx9j8ozVqg0tIQ5OGUKgBrSzhiloqarFSSIIgjSJbK90KFa5w/7iiiEIRtPQHBaGQYOhGalIh/oihNL+Q2ovXaiLWaGyrV1KbEJMrKTb9QxOTp3/sSbvqznfGmTNzZvd5v2DZmfOcHw/DfvacmXPOfB0RApDPGU03AKAZhB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFJnDnJjtrmcEOiziHAn8/W057d9re0/2X7T9p29rAvAYLnba/ttz5H0Z0nXSDog6UVJayJiX2EZ9vxAnw1iz79C0psR8VZEHJe0SdLKHtYHYIB6Cf9CSX+d8vxANe0jbI/b3m17dw/bAlCzvn/gFxFrJa2VOOwHhkkve/6DkhZNef75ahqAGaCX8L8oaZntJbbnSvqWpC31tAWg37o+7I+ID23fLmmbpDmS1kfEa7V1BqCvuj7V19XGeM8P9N1ALvIBMHMRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5DUQIfoBgZp586dLWtXXXVVcdmxsbFifcOGDV31NEzY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUj2d57e9X9IxSSclfRgRo3U0BXTimWeeKdYvu+yylrVTp04Vlx3k6NVNqeMinysj4t0a1gNggDjsB5LqNfwhabvtPbbH62gIwGD0eth/eUQctH2BpB22/xgRz06dofqnwD8GYMj0tOePiIPV7yOSnpC0Ypp51kbEKB8GAsOl6/DbPsv2Z08/lvR1Sa/W1RiA/urlsH9E0hO2T6/n0Yj4TS1dAei7rsMfEW9JuqjGXoCPuPvuu4v1Sy+9tFifM2dOy9pjjz1WXHbz5s3F+mzAqT4gKcIPJEX4gaQIP5AU4QeSIvxAUh7krYu2Z/99kujYqlWrivWNGzcW63Pnzi3W9+7d27J2xRVXFJc9duxYsT7MIsKdzMeeH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSYohu9NWiRYta1u69997isu3O4x89erRYv+eee1rWZvJ5/Lqw5weSIvxAUoQfSIrwA0kRfiApwg8kRfiBpLifHz1ZseITgzR9xLp161rWli9f3tO2b7rppmJ906ZNPa1/puJ+fgBFhB9IivADSRF+ICnCDyRF+IGkCD+QVNv7+W2vl/RNSUciYnk17XxJv5K0WNJ+STdExN/61yaacvPNNxfrExMTxXrpOpL33nuvuOzOnTuL9W3bthXrKOtkz/8LSdd+bNqdknZFxDJJu6rnAGaQtuGPiGclffwrU1ZKOv0vf0JSeegVAEOn2/f8IxFxqHr8jqSRmvoBMCA9f4dfRETpmn3b45LGe90OgHp1u+c/bHuBJFW/j7SaMSLWRsRoRIx2uS0AfdBt+LdIGqsej0l6qp52AAxK2/Db3ijpfyV9yfYB27dKekDSNbbfkHR19RzADML9/MmNjJQ/q92xY0ex3u6e/NLf14YNG4rL3nLLLcU6psf9/ACKCD+QFOEHkiL8QFKEH0iK8ANJMUT3LHfuuecW69u3by/WL7zwwp62XxoKe8uWLT2tG71hzw8kRfiBpAg/kBThB5Ii/EBShB9IivADSXFL7yy3cOHCYv3tt9/uaf12+e7RefPmtayVrgFA97ilF0AR4QeSIvxAUoQfSIrwA0kRfiApwg8kxf38s8D8+fNb1p5++unisu3O07fz/PPPF+vHjx/vaf3oH/b8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5BU2/P8ttdL+qakIxGxvJp2n6TvSvr/ara7IuLX/WoSZQ899FDL2kUXXVRctt33OTz33HPF+tVXX12sf/DBB8U6mtPJnv8Xkq6dZvpPI+Li6ofgAzNM2/BHxLOSjg6gFwAD1Mt7/tttv2J7ve3zausIwEB0G/6fS1oq6WJJhyT9uNWMtsdt77a9u8ttAeiDrsIfEYcj4mREnJK0TtKKwrxrI2I0Ika7bRJA/boKv+0FU56ulvRqPe0AGJROTvVtlPQ1SfNtH5B0r6Sv2b5YUkjaL+l7fewRQB+0DX9ErJlm8sN96AUtlO7Xl6SlS5d2ve4TJ04U6w8++GCxznn8mYsr/ICkCD+QFOEHkiL8QFKEH0iK8ANJ8dXdQ+CCCy4o1h999NFi/ZJLLmlZe//994vL3nbbbcX61q1bi3XMXOz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApzvMPgdWrVxfrV155ZdfrfuGFF4r1Rx55pOt1Y2Zjzw8kRfiBpAg/kBThB5Ii/EBShB9IivADSXGefwDWrJnu28//rd3XY7dTGkb7xhtv7GndmL3Y8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUo6I8gz2IkkbJI1ICklrI+Jnts+X9CtJiyXtl3RDRPytzbrKG5uh5s2bV6zv2bOnWF+yZElP27/++utb1p588sme1o2ZJyLcyXyd7Pk/lPRfEfFlSf8h6fu2vyzpTkm7ImKZpF3VcwAzRNvwR8ShiHipenxM0uuSFkpaKWmimm1C0qp+NQmgfp/qPb/txZK+Iun3kkYi4lBVekeTbwsAzBAdX9tv+2xJmyXdERF/t//9tiIiotX7edvjksZ7bRRAvTra89v+jCaD/8uIeLyafNj2gqq+QNKR6ZaNiLURMRoRo3U0DKAebcPvyV38w5Jej4ifTCltkTRWPR6T9FT97QHol04O+y+TdLOkvbZfrqbdJekBSY/ZvlXSXyTd0J8Wh9/KlSuL9V5P5bVzzjnn9HX9mJ3ahj8i/kdSq/OGV9XbDoBB4Qo/ICnCDyRF+IGkCD+QFOEHkiL8QFJ8dXcNTpw4UayfOnWqWD/jjPL/4JMnTxbry5YtK9aB6bDnB5Ii/EBShB9IivADSRF+ICnCDyRF+IGk2n51d60bm6Vf3d3Ovn37ivUzzyxfbnH//fcX6xMTE8U6cqnzq7sBzEKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU5/mBWYbz/ACKCD+QFOEHkiL8QFKEH0iK8ANJEX4gqbbht73I9jO299l+zfZ/VtPvs33Q9svVz3X9bxdAXdpe5GN7gaQFEfGS7c9K2iNplaQbJP0jIn7U8ca4yAfou04v8mk7Yk9EHJJ0qHp8zPbrkhb21h6Apn2q9/y2F0v6iqTfV5Nut/2K7fW2z2uxzLjt3bZ399QpgFp1fG2/7bMl/VbS/RHxuO0RSe9KCkk/0ORbg++0WQeH/UCfdXrY31H4bX9G0lZJ2yLiJ9PUF0vaGhHL26yH8AN9VtuNPbYt6WFJr08NfvVB4GmrJb36aZsE0JxOPu2/XNLvJO2VdHqs6bskrZF0sSYP+/dL+l714WBpXez5gT6r9bC/LoQf6D/u5wdQRPiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iq7Rd41uxdSX+Z8nx+NW0YDWtvw9qXRG/dqrO3L3Q640Dv5//Exu3dETHaWAMFw9rbsPYl0Vu3muqNw34gKcIPJNV0+Nc2vP2SYe1tWPuS6K1bjfTW6Ht+AM1pes8PoCGNhN/2tbb/ZPtN23c20UMrtvfb3luNPNzoEGPVMGhHbL86Zdr5tnfYfqP6Pe0waQ31NhQjNxdGlm70tRu2Ea8Hfthve46kP0u6RtIBSS9KWhMR+wbaSAu290sajYjGzwnb/qqkf0jacHo0JNs/lHQ0Ih6o/nGeFxH/PSS93adPOXJzn3prNbL0t9Xga1fniNd1aGLPv0LSmxHxVkQcl7RJ0soG+hh6EfGspKMfm7xS0kT1eEKTfzwD16K3oRARhyLiperxMUmnR5Zu9LUr9NWIJsK/UNJfpzw/oOEa8jskbbe9x/Z4081MY2TKyEjvSBppsplptB25eZA+NrL00Lx23Yx4XTc+8PukyyPiEknfkPT96vB2KMXke7ZhOl3zc0lLNTmM2yFJP26ymWpk6c2S7oiIv0+tNfnaTdNXI69bE+E/KGnRlOefr6YNhYg4WP0+IukJTb5NGSaHTw+SWv0+0nA//xIRhyPiZESckrRODb521cjSmyX9MiIeryY3/tpN11dTr1sT4X9R0jLbS2zPlfQtSVsa6OMTbJ9VfRAj22dJ+rqGb/ThLZLGqsdjkp5qsJePGJaRm1uNLK2GX7uhG/E6Igb+I+k6TX7i/3+S7m6ihxZ9fVHSH6qf15ruTdJGTR4GntDkZyO3SvqcpF2S3pC0U9L5Q9TbI5oczfkVTQZtQUO9Xa7JQ/pXJL1c/VzX9GtX6KuR140r/ICk+MAPSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBS/wQIaCn7CGJwiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.imshow(o[0].reshape((28, 28)), cmap='gray')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(xtrain[0].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
